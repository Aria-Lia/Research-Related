\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{krizhevsky2012imagenet}
\citation{long2015fully}
\citation{isola2017image}
\citation{tulsiani2017learning}
\citation{shapenet2015}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{brf}{\backcite{krizhevsky2012imagenet}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{long2015fully}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{isola2017image}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{tulsiani2017learning}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{shapenet2015}{{1}{1}{section.1}}}
\citation{o2014exploratory}
\citation{campbell2014learning}
\citation{Balashova18}
\citation{suveeranont2010example}
\citation{phan2015flexyfont}
\citation{azadi2018multi}
\citation{upchurch2016z}
\citation{kim2013learning}
\citation{shen2012structure}
\citation{ovsjanikov2011exploration}
\citation{schulz2017retrieval}
\citation{umetani2012guided}
\citation{fuentes2015visual}
\citation{seitz2006comparison}
\citation{su2015multi}
\citation{fan2017point}
\citation{choy20163d}
\citation{dai2017shape}
\citation{stutz2018learning}
\citation{park2019deepsdf}
\citation{liao2018deep}
\citation{li2018supervised}
\citation{niu2018im2struct}
\citation{ganapathi2018parsing}
\citation{tulsiani2017learning}
\citation{groueix2018atlasnet}
\citation{borgefors1984distance}
\citation{tulsiani2017learning}
\citation{fan2017point}
\citation{liu2004hand}
\citation{groueix2018atlasnet}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{brf}{\backcite{o2014exploratory}{{2}{2}{section*.1}}}
\@writefile{brf}{\backcite{campbell2014learning,Balashova18}{{2}{2}{section*.1}}}
\@writefile{brf}{\backcite{suveeranont2010example,phan2015flexyfont}{{2}{2}{section*.1}}}
\@writefile{brf}{\backcite{azadi2018multi,upchurch2016z}{{2}{2}{section*.1}}}
\@writefile{brf}{\backcite{kim2013learning,shen2012structure,ovsjanikov2011exploration}{{2}{2}{section*.2}}}
\@writefile{brf}{\backcite{schulz2017retrieval,umetani2012guided}{{2}{2}{section*.2}}}
\@writefile{brf}{\backcite{fuentes2015visual,seitz2006comparison,su2015multi}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{fan2017point,choy20163d}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{dai2017shape,stutz2018learning}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{park2019deepsdf}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{liao2018deep}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{li2018supervised, niu2018im2struct}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{ganapathi2018parsing}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{tulsiani2017learning}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{groueix2018atlasnet}{{2}{2}{section*.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Preliminaries}{2}{section.3}\protected@file@percent }
\newlabel{sec-prelim}{{3}{2}{\hskip -1em.~Preliminaries}{section.3}{}}
\newlabel{eq:chamferdir}{{1}{2}{\hskip -1em.~Preliminaries}{equation.3.1}{}}
\newlabel{eq:chamfer}{{2}{2}{\hskip -1em.~Preliminaries}{equation.3.2}{}}
\@writefile{brf}{\backcite{borgefors1984distance}{{2}{3}{equation.3.2}}}
\@writefile{brf}{\backcite{tulsiani2017learning, fan2017point, liu2004hand, groueix2018atlasnet}{{2}{3}{equation.3.2}}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:chamfer-sampling}{{1a}{3}{Sampling\relax }{figure.caption.4}{}}
\newlabel{sub@fig:chamfer-sampling}{{a}{3}{Sampling\relax }{figure.caption.4}{}}
\newlabel{fig:chamfer-normals}{{1b}{3}{Alignment\relax }{figure.caption.4}{}}
\newlabel{sub@fig:chamfer-normals}{{b}{3}{Alignment\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Drawbacks of Chamfer distance. In (a), sampling from B\`ezier curve $B$ (blue) by uniformly sampling in parameter space yields disproportionately many points at the high-curvature area, resulting in a low Chamfer distance to the segments of $A$ (orange) despite geometric dissimilarity. In (b), two sets of nearly-orthogonal line segments have near-zero Chamfer distance despite misaligned normals.\vspace  {-0.1in}\relax }}{3}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Method}{3}{section.4}\protected@file@percent }
\newlabel{sec-method}{{4}{3}{\hskip -1em.~Method}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}General Distance Field Loss}{3}{subsection.4.1}\protected@file@percent }
\newlabel{gdf-loss}{{4}{3}{\hskip -1em.~General Distance Field Loss}{equation.4.4}{}}
\newlabel{eq:generalloss}{{5}{3}{\hskip -1em.~General Distance Field Loss}{equation.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An overview of our pipelines---font vectorization (green) and volumetric primitive prediction (orange).\vspace  {-0.1in}\relax }}{3}{figure.caption.5}\protected@file@percent }
\newlabel{fig:architecture}{{2}{3}{An overview of our pipelines---font vectorization (green) and volumetric primitive prediction (orange).\vspace {-0.1in}\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Surface Loss}{3}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.\nobreakspace  {}Global Loss}{3}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.\nobreakspace  {}Normal Alignment Loss}{3}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}\hskip -1em.\nobreakspace  {}Final Loss Function}{3}{subsection.4.5}\protected@file@percent }
\citation{he2016identity}
\citation{he2016deep}
\citation{yu2015multi}
\citation{chen2018deeplab}
\citation{clevert2015fast}
\citation{ba2016layer}
\citation{kingma2014adam}
\citation{qin2006real}
\citation{zhu2017toward}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}\hskip -1em.\nobreakspace  {}Network architecture}{4}{subsection.4.6}\protected@file@percent }
\@writefile{brf}{\backcite{he2016identity,he2016deep}{{4}{4.6}{subsection.4.6}}}
\@writefile{brf}{\backcite{yu2015multi, chen2018deeplab}{{4}{4.6}{subsection.4.6}}}
\@writefile{brf}{\backcite{clevert2015fast}{{4}{4.6}{subsection.4.6}}}
\@writefile{brf}{\backcite{ba2016layer}{{4}{4.6}{subsection.4.6}}}
\@writefile{brf}{\backcite{kingma2014adam}{{4}{4.6}{subsection.4.6}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}2D: Font Exploration and Manipulation}{4}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Approach}{4}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Primitives}{4}{subsubsection.5.1.1}\protected@file@percent }
\newlabel{curve-dist}{{11}{4}{Primitives}{equation.5.11}{}}
\@writefile{brf}{\backcite{qin2006real}{{4}{5.1.1}{equation.5.11}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Templates}{4}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{brf}{\backcite{zhu2017toward}{{4}{5.1.2}{subsubsection.5.1.2}}}
\newlabel{fig:abc-plain}{{3a}{5}{Plain font glyphs\relax }{figure.caption.6}{}}
\newlabel{sub@fig:abc-plain}{{a}{5}{Plain font glyphs\relax }{figure.caption.6}{}}
\newlabel{fig:abc-decorative}{{3b}{5}{Decorative font glyphs\relax }{figure.caption.6}{}}
\newlabel{sub@fig:abc-decorative}{{b}{5}{Decorative font glyphs\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Vectorization of various glyphs. For each we show the raster input (top left, gray) along with the vectorization (colored curves) superimposed. When the input has simple structure (a), we recover an accurate vectorization. For fonts with decorative details (b), our method places template curves to capture overall structure. Results are taken from the test dataset.\vspace  {-.18in}\relax }}{5}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Glyphs with corresponding predicted curves rendered with predicted stroke thickness. The network thickens curves of to account for stylistic details.\vspace  {-0.1in}\relax }}{5}{figure.caption.7}\protected@file@percent }
\newlabel{fig:abc-stroke}{{4}{5}{Glyphs with corresponding predicted curves rendered with predicted stroke thickness. The network thickens curves of to account for stylistic details.\vspace {-0.1in}\relax }{figure.caption.7}{}}
\newlabel{fig:templates-abc}{{5a}{5}{Letter templates\relax }{figure.caption.8}{}}
\newlabel{sub@fig:templates-abc}{{a}{5}{Letter templates\relax }{figure.caption.8}{}}
\newlabel{fig:simple_templates}{{5b}{5}{Simple templates\relax }{figure.caption.8}{}}
\newlabel{sub@fig:simple_templates}{{b}{5}{Simple templates\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Font glyph templates. These determine the connectivity and initialize the placement of the predicted curves.\vspace  {-0.1in}\relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:templates}{{5}{5}{Font glyph templates. These determine the connectivity and initialize the placement of the predicted curves.\vspace {-0.1in}\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Nearest neighbors for a glyph in curve space, sorted by proximity. The query glyph is in orange.\vspace  {-0.1in}\relax }}{5}{figure.caption.9}\protected@file@percent }
\newlabel{fig:abc-nn}{{6}{5}{Nearest neighbors for a glyph in curve space, sorted by proximity. The query glyph is in orange.\vspace {-0.1in}\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Experiments}{5}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Vectorization}{5}{subsubsection.5.2.1}\protected@file@percent }
\citation{azadi2018multi}
\citation{azadi2018multi}
\citation{azadi2018multi}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Interpolating between fonts in curve space. The start and end are shown in orange and blue, resp., and nearest-neighbor glyphs to linear interpolants are shown in order.\vspace  {-0.1in}\relax }}{6}{figure.caption.10}\protected@file@percent }
\newlabel{fig:abc-interpolation}{{7}{6}{Interpolating between fonts in curve space. The start and end are shown in orange and blue, resp., and nearest-neighbor glyphs to linear interpolants are shown in order.\vspace {-0.1in}\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Number of examples per quantized loss value. We visualize the input and predicted curves for several outliers.\vspace  {-0.1in}\relax }}{6}{figure.caption.11}\protected@file@percent }
\newlabel{fig:abc-loss}{{8}{6}{Number of examples per quantized loss value. We visualize the input and predicted curves for several outliers.\vspace {-0.1in}\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Retrieval and Exploration}{6}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Style and Structure Mixing}{6}{subsubsection.5.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces User-guided font exploration. At each edit, the nearest-neighbor glyph is displayed on the bottom. This lets the user explore the dataset through geometric refinements.\vspace  {-0.1in}\relax }}{6}{figure.caption.12}\protected@file@percent }
\newlabel{fig:abc-exploration}{{9}{6}{User-guided font exploration. At each edit, the nearest-neighbor glyph is displayed on the bottom. This lets the user explore the dataset through geometric refinements.\vspace {-0.1in}\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Mixing of style (columns) and structure (rows) of the \emph  {A} glyph from different fonts. We deform each starting glyph (orange) into the structure of each target glyph (blue).\vspace  {-0.1in}\relax }}{6}{figure.caption.13}\protected@file@percent }
\newlabel{fig:abc-analogies}{{10}{6}{Mixing of style (columns) and structure (rows) of the \emph {A} glyph from different fonts. We deform each starting glyph (orange) into the structure of each target glyph (blue).\vspace {-0.1in}\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Repair}{6}{subsubsection.5.2.4}\protected@file@percent }
\@writefile{brf}{\backcite{azadi2018multi}{{6}{5.2.4}{subsubsection.5.2.4}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.5}Comparison and Ablation Study}{6}{subsubsection.5.2.5}\protected@file@percent }
\citation{shapenet2015}
\citation{dai2017shape}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Vectorization of GAN-generated fonts from \cite  {azadi2018multi}.\vspace  {-0.1in}\relax }}{7}{figure.caption.14}\protected@file@percent }
\@writefile{brf}{\backcite{azadi2018multi}{{7}{11}{figure.caption.14}}}
\newlabel{fig:abc-gan}{{11}{7}{Vectorization of GAN-generated fonts from \cite {azadi2018multi}.\vspace {-0.1in}\relax }{figure.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison between subsets of our full loss as well as standard Chamfer distance. Average error is Chamfer distance (in pixels on a $128\tmspace  -\thinmuskip {.1667em}\times \tmspace  -\thinmuskip {.1667em}128$ image) between predicted curves and ground truth, with points sampled uniformly. This demonstrates advantages of our loss over Chamfer distance and shows how each loss term contributes to the results.\vspace  {-0.1in}\relax }}{7}{table.caption.15}\protected@file@percent }
\newlabel{table:timing}{{1}{7}{Comparison between subsets of our full loss as well as standard Chamfer distance. Average error is Chamfer distance (in pixels on a $128\!\times \!128$ image) between predicted curves and ground truth, with points sampled uniformly. This demonstrates advantages of our loss over Chamfer distance and shows how each loss term contributes to the results.\vspace {-0.1in}\relax }{table.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparisons to missing terms and Chamfer.\vspace  {-0.1in}\relax }}{7}{figure.caption.16}\protected@file@percent }
\newlabel{fig:abc-ablation}{{12}{7}{Comparisons to missing terms and Chamfer.\vspace {-0.1in}\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}3D: Volumetric Primitive Prediction}{7}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}\hskip -1em.\nobreakspace  {}Approach}{7}{subsection.6.1}\protected@file@percent }
\citation{tulsiani2017learning}
\citation{tulsiani2017learning}
\citation{tulsiani2017learning}
\citation{bessmeltsev2019vectorization}
\citation{blinn1982generalization}
\citation{du2018inversecsg}
\bibstyle{ieee}
\bibdata{bibliography}
\bibcite{azadi2018multi}{1}
\newlabel{fig:cube-seg}{{13c}{8}{Shape COSEG chairs\relax }{figure.caption.18}{}}
\newlabel{sub@fig:cube-seg}{{c}{8}{Shape COSEG chairs\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Cuboid shape abstractions on test set inputs. In (a) and (b), we show ShapeNet chairs and airplanes. In (c), we show Shape COSEG chair segmentation. We show each input model (left) next to the cuboid representation (right).\vspace  {-0.1in}\relax }}{8}{figure.caption.18}\protected@file@percent }
\newlabel{fig:cube-abstractions}{{13}{8}{Cuboid shape abstractions on test set inputs. In (a) and (b), we show ShapeNet chairs and airplanes. In (c), we show Shape COSEG chair segmentation. We show each input model (left) next to the cuboid representation (right).\vspace {-0.1in}\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}\hskip -1em.\nobreakspace  {}Experiments}{8}{subsection.6.2}\protected@file@percent }
\@writefile{brf}{\backcite{shapenet2015}{{8}{6.2}{subsection.6.2}}}
\@writefile{brf}{\backcite{dai2017shape}{{8}{6.2}{subsection.6.2}}}
\@writefile{brf}{\backcite{tulsiani2017learning}{{8}{6.2}{section*.17}}}
\@writefile{brf}{\backcite{tulsiani2017learning}{{8}{6.2}{section*.19}}}
\@writefile{brf}{\backcite{tulsiani2017learning}{{8}{6.2}{section*.19}}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Single view reconstruction using different primitives and boolean operations.\vspace  {-0.1in}\relax }}{8}{figure.caption.21}\protected@file@percent }
\newlabel{fig:csg}{{14}{8}{Single view reconstruction using different primitives and boolean operations.\vspace {-0.1in}\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}\hskip -1em.\nobreakspace  {}Conclusion}{8}{section.7}\protected@file@percent }
\@writefile{brf}{\backcite{bessmeltsev2019vectorization}{{8}{7}{section.7}}}
\@writefile{brf}{\backcite{blinn1982generalization}{{8}{7}{section.7}}}
\@writefile{brf}{\backcite{du2018inversecsg}{{8}{7}{section.7}}}
\bibcite{ba2016layer}{2}
\bibcite{Balashova18}{3}
\bibcite{bessmeltsev2019vectorization}{4}
\bibcite{blinn1982generalization}{5}
\bibcite{borgefors1984distance}{6}
\bibcite{campbell2014learning}{7}
\bibcite{shapenet2015}{8}
\bibcite{chen2018deeplab}{9}
\bibcite{choy20163d}{10}
\bibcite{clevert2015fast}{11}
\bibcite{dai2017shape}{12}
\bibcite{du2018inversecsg}{13}
\bibcite{fan2017point}{14}
\bibcite{fuentes2015visual}{15}
\bibcite{ganapathi2018parsing}{16}
\bibcite{groueix2018atlasnet}{17}
\bibcite{he2016deep}{18}
\bibcite{he2016identity}{19}
\bibcite{isola2017image}{20}
\bibcite{kim2013learning}{21}
\bibcite{kingma2014adam}{22}
\bibcite{krizhevsky2012imagenet}{23}
\bibcite{li2018supervised}{24}
\bibcite{liao2018deep}{25}
\bibcite{liu2004hand}{26}
\bibcite{long2015fully}{27}
\bibcite{niu2018im2struct}{28}
\bibcite{o2014exploratory}{29}
\bibcite{ovsjanikov2011exploration}{30}
\bibcite{park2019deepsdf}{31}
\bibcite{phan2015flexyfont}{32}
\bibcite{qin2006real}{33}
\bibcite{schulz2017retrieval}{34}
\bibcite{seitz2006comparison}{35}
\bibcite{shen2012structure}{36}
\bibcite{stutz2018learning}{37}
\bibcite{su2015multi}{38}
\bibcite{suveeranont2010example}{39}
\bibcite{tulsiani2017learning}{40}
\bibcite{umetani2012guided}{41}
\bibcite{upchurch2016z}{42}
\bibcite{yu2015multi}{43}
\bibcite{zhu2017toward}{44}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Glyph nearest neighbors in curve space.\relax }}{11}{figure.caption.23}\protected@file@percent }
\newlabel{fig:sup-nn}{{15}{11}{Glyph nearest neighbors in curve space.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Interpolations between fonts in curve space.\relax }}{11}{figure.caption.24}\protected@file@percent }
\newlabel{fig:sup-interpolation}{{16}{11}{Interpolations between fonts in curve space.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Distance field loss comparisons.\relax }}{12}{figure.caption.25}\protected@file@percent }
\newlabel{fig:sup-ablation}{{17}{12}{Distance field loss comparisons.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Local loss (smoothed) over the first 4,000 iterations of training with and without global loss in the objective function. The global loss term results in faster convergence.\vspace  {-0.1in}\relax }}{12}{figure.caption.26}\protected@file@percent }
\newlabel{fig:loss_convergence}{{18}{12}{Local loss (smoothed) over the first 4,000 iterations of training with and without global loss in the objective function. The global loss term results in faster convergence.\vspace {-0.1in}\relax }{figure.caption.26}{}}
\newlabel{fig:sup-chairs}{{\caption@xref {fig:sup-chairs}{ on input line 40}}{13}{\hskip -1em.~Conclusion}{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Cuboid reconstructions of ShapeNet chairs.\relax }}{13}{figure.caption.27}\protected@file@percent }
\newlabel{fig:sup-airplanes}{{\caption@xref {fig:sup-airplanes}{ on input line 47}}{14}{\hskip -1em.~Conclusion}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Cuboid reconstructions of ShapeNet airplanes.\relax }}{14}{figure.caption.28}\protected@file@percent }
\newlabel{fig:sup-seg}{{\caption@xref {fig:sup-seg}{ on input line 54}}{15}{\hskip -1em.~Conclusion}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Cuboid segmentation of Shape COSEG chairs.\relax }}{15}{figure.caption.29}\protected@file@percent }
